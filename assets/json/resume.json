{"basics":{"name":"Cheng Chien","label":"","image":"","email":"chengchien1999@gmail.com","phone":"(886) 989819022","url":"https://www.linkedin.com/in/cchien1999","summary":"I am a computer science student with a robust background in machine learning, specializing in image coding for machines and multimodal large language models.","location":{"address":"2712 Broadway St","postalCode":"CA 94115","city":"San Francisco","countryCode":"US","region":"California"},"profiles":[{"network":"Twitter","username":"AlbertEinstein","url":"https://twitter.com/AlbertEinstein"}]},"//work":[{"name":"Institute for Advanced Study, Princeton University","position":"Professor of Theoretical Physics","url":"https://example.com","startDate":"1933-01-01","endDate":"1955-01-01","summary":"Teaching at Palmer Physical Laboratory (now 302 Frist Campus Center). While not a professor at Princeton, I associated with the physics professors and continued to give lectures on campus.","highlights":["Relativity"]}],"//volunteer":[{"organization":"People's Climate March","position":"Lead Organizer","url":"https://example.com","startDate":"2014-04-01","endDate":"2015-07-01","summary":"Lead organizer for the New York City branch of the People's Climate March, the largest climate march in history.","highlights":["Awarded 'Climate Hero' award by Greenpeace for my efforts organizing the march.","Men of the year 2014 by Time magazine"]}],"education":[{"institution":"National Yang Ming Chiao Tung University, Taiwan","location":"Taiwan","url":"https://www.nycu.edu.tw/nycu/en/index","area":"Computer Sciences","studyType":"Master","startDate":"2022","endDate":"2024","score":"4.19/4.3","//courses":["Theory of Relativity"]},{"institution":"National Sun Yat-sen University, Taiwan","location":"Taiwan","url":"https://www.nsysu.edu.tw/?Lang=en","area":"Computer Science and Engineering","studyType":"Bachelor","startDate":"2018","endDate":"2022","score":"4.05/4.3","//courses":["Theory of Relativity"]}],"//certificates":[{"name":"Machine Learning","date":"2018-01-01","issuer":"Stanford University","url":"https://example.com","icon":"fa-solid fa-location-dot"},{"name":"Quantum Computing","date":"2018-01-01","issuer":"Stanford University","url":"https://example.com","icon":"fa-solid fa-tag"},{"name":"Quantum Information","date":"2018-01-01","issuer":"Stanford University","url":"https://example.com","icon":"fa-solid fa-envelope"},{"name":"Quantum Cryptography","date":"2018-01-01","issuer":"Stanford University","url":"https://example.com","icon":"fa-solid fa-hashtag"},{"name":"Quantum Communication","date":"2018-01-01","issuer":"Stanford University","url":"https://example.com","icon":"fa-solid fa-calendar"},{"name":"Quantum Teleportation","date":"2018-01-01","issuer":"Stanford University","url":"https://example.com","icon":"fa-solid fa-clipboard-check"}],"skills":[{"name":"Programming","level":"Master","icon":"fa-solid fa-hashtag","keywords":["Python","C","C++"]},{"name":"Machine Learning","level":"Master","icon":"fa-solid fa-hashtag","keywords":["Pytorch"]}],"languages":[{"language":"Mandarin","fluency":"Native speaker","icon":""},{"language":"English","fluency":"Fluent","icon":""}],"interests":[{"name":"Machine Learning","icon":"fa-solid fa-tag","keywords":["Image coding for machine","Multimodal Large language models"]}],"//references":[{"name":"Professor John Doe","icon":"fa-solid fa-laptop","reference":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam condimentum, diam quis convallis euismod, arcu mi ullamcorper lorem, a vestibulum nunc magna at sem. Sed in risus ac felis varius blandit. D"},{"name":"Professor John Doe","icon":"fa-solid fa-thumbtack","reference":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aliquam condimentum, diam quis convallis euismod, arcu mi ullamcorper lorem, a vestibulum nunc magna at sem. Sed in risus ac felis varius blandit. D"}],"awards":[{"title":"Taiwan Imaging-Tek Corporation Scholarship","awarder":"Taiwan Imaging-Tek Corporation"},{"title":"Excellent Student Awards","awarder":"National Sun Yat-sen University"},{"title":"Calculus Contest Awards","awarder":"National Sun Yat-sen University"},{"title":"Cathay Financial Holdings Customer-Children Scholarship","awarder":"Cathay Financial Holdings"},{"title":"New Taipei City Apartment Building Management Services Occupational Union Scholarship","awarder":"New Taipei City Apartment Building Management Services Occupational Union"},{"//title":"Nobel Prize in Physics","//date":"1921-11-01","//awarder":"Royal Swedish Academy of Sciences","//url":"https://www.nobelprize.org/prizes/physics/1921/einstein/biographical/","//summary":"The Nobel Prizes are five separate prizes that, according to Alfred Nobel's will of 1895, are awarded to 'those who, during the preceding year, have conferred the greatest benefit to humankind.'"}],"publications":[{"name":"ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck","publisher":"Chia-Hao Kao, Cheng Chien, Yu-Jen Tseng, Yi-Hsin Chen, Alessandro Gnutti, Shao-Yuan Lo, Wen-Hsiao Peng, Riccardo Leonardi","releaseDate":"2024","url":"https://arxiv.org/abs/2407.19651","summary":"This paper presents the first-ever study of adapting compressed image latents to suit the needs of downstream vision tasks that adopt Multimodal Large Language Models (MLLMs). MLLMs have extended the success of large language models to modalities (e.g. images) beyond text, but their billion scale hinders deployment on resource-constrained end devices. While cloud-hosted MLLMs could be available, transmitting raw, uncompressed images captured by end devices to the cloud requires an efficient image compression system. To address this, we focus on emerging neural image compression and propose a novel framework with a lightweight transform-neck and a surrogate loss to adapt compressed image latents for MLLM-based vision tasks. The proposed framework is generic and applicable to multiple application scenarios, where the neural image codec can be (1) pre-trained for human perception without updating, (2) fully updated for joint human and machine perception, or (3) fully updated for only machine perception. The transform-neck trained with the surrogate loss is universal, for it can serve various downstream vision tasks enabled by a variety of MLLMs that share the same visual encoder. Our framework has the striking feature of excluding the downstream MLLMs from training the transform-neck, and potentially the neural image codec as well. This stands out from most existing coding for machine approaches that involve downstream networks in training and thus could be impractical when the networks are MLLMs. Extensive experiments on different neural image codecs and various MLLM-based vision tasks show that our method achieves great rate-accuracy performance with much less complexity, demonstrating its effectiveness."},{"name":"TransTIC: Transferring Transformer-based Image Compression from Human Visualization to Machine Perception","publisher":"Yi-Hsin Chen, Ying-Chieh Weng, Chia-Hao  Kao, Cheng Chien, Wei-Chen Chiu, and Wen-Hsiao Peng","releaseDate":"2023","url":"https://arxiv.org/abs/2306.05085","summary":"This work aims for transferring a Transformer-based image compression codec from human perception to machine perception without fine-tuning the codec. We propose a transferable Transformer-based image compression framework, termed TransTIC. Inspired by visual prompt tuning, TransTIC adopts an instance-specific prompt generator to inject instance-specific prompts to the encoder and task-specific prompts to the decoder. Extensive experiments show that our proposed method is capable of transferring the base codec to various machine tasks and outperforms the competing methods significantly. To our best knowledge, this work is the first attempt to utilize prompting on the low-level image compression task."},{"name":"Transformer-based Image Compression with Variable Image Quality Objectives","publisher":"Chia-Hao Kao* , Yi-Hsin Chen* , Cheng Chien, Wei-Chen Chiu, and Wen-Hsiao Peng","releaseDate":"2023","url":"https://arxiv.org/abs/2309.12717","summary":"This paper presents a Transformer-based image compression system that allows for a variable image quality objective according to the user's preference. Optimizing a learned codec for different quality objectives leads to reconstructed images with varying visual characteristics. Our method provides the user with the flexibility to choose a trade-off between two image quality objectives using a single, shared model. Motivated by the success of prompt-tuning techniques, we introduce prompt tokens to condition our Transformer-based autoencoder. These prompt tokens are generated adaptively based on the user's preference and input image through learning a prompt generation network. Extensive experiments on commonly used quality metrics demonstrate the effectiveness of our method in adapting the encoding and/or decoding processes to a variable quality objective. While offering the additional flexibility, our proposed method performs comparably to the single-objective methods in terms of rate-distortion performance."},{"name":"Learned hierarchical b-frame coding with adaptive feature modulation for yuv 4: 2: 0 contente","publisher":"Mu-Jung Chen , Hong-Sheng Xie , Cheng Chien, Wen-Hsiao Peng, and Hsueh-Ming Hang","releaseDate":"2023","url":"https://arxiv.org/abs/2212.14187","summary":"This paper introduces a learned hierarchical B-frame coding scheme in response to the Grand Challenge on Neural Network-based Video Coding at ISCAS 2023. We address specifically three issues, including (1) B-frame coding, (2) YUV 4:2:0 coding, and (3) content-adaptive variable-rate coding with only one single model. Most learned video codecs operate internally in the RGB domain for P-frame coding. B-frame coding for YUV 4:2:0 content is largely under-explored. In addition, while there have been prior works on variable-rate coding with conditional convolution, most of them fail to consider the content information. We build our scheme on conditional augmented normalized flows (CANF). It features conditional motion and inter-frame codecs for efficient B-frame coding. To cope with YUV 4:2:0 content, two conditional inter-frame codecs are used to process the Y and UV components separately, with the coding of the UV components conditioned additionally on the Y component. Moreover, we introduce adaptive feature modulation in every convolutional layer, taking into account both the content information and the coding levels of B-frames to achieve content-adaptive variable-rate coding. Experimental results show that our model outperforms x265 and the winner of last year's challenge on commonly used datasets in terms of PSNR-YUV."}],"//projects":[{"name":"Quantum Computing","summary":"Quantum computing is the use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. Computers that perform quantum computations are known as quantum computers.","highlights":["Quantum Teleportation","Quantum Cryptography"],"startDate":"2018-01-01","endDate":"2018-01-01","url":"https://example.com"}]}